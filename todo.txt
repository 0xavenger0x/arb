Core arithmetic
-------------------------------------------------------------------------------

* Consider changing the interface of functions such as X_set_Y, X_neg_Y
  to always take a precision parameter (and get rid of X_set_round_Y,
  X_neg_Y etc.). Perhaps have X_setexact_Y methods for convenience,
  or make an exception for _set_ in particular.

* Make sure that excessive shifts in add/sub are detected
  with exact precision. Write tests for correctness of overlaps/contains
  in huge-exponent cases.

* Double-check correctness of add/sub code with large shifts (rounding x+eps).

* Work out semantics for comparisons/overlap/containment checks
  when NaNs are involved, and write test code.

* Native string conversion code instead of relying on mpfr (so we can have
  big exponents, etc.).

* Add functions for sloppy arithmetic (non-exact rounding). This could be
  used to speed up some ball operations with inexact output, where we don't
  need the best possible result, just a correct error bound.

* Write functions that ignore the possibility that exponents might be
  large, and use where appropriate (e.g. polynomial and matrix multiplication
  where one bounds magnitudes in an initial pass).

* Rewrite fmprb_div (similar to fmprb_mul)


Polynomial and power series arithmetic
-------------------------------------------------------------------------------

* Verify that mullow and power series methods always truncate the inputs to
  length n.

* Handle all input of special form ax^n + b quickly in composition and powering.

* Implemented the addition and convlution methods for Taylor shifts.

* Add polynomial mulmid, and use in Newton iteration

* Tune basecase/Newton selection for exp/sin/cos series (the basecase
  algorithms are more stable, and faster for quite large n)

* Look at using the exponential to compute the complex sine/cosine series

* Improve block multiplication, e.g. by discarding blocks that don't contribute
  to the result, and scaling individual blocks.


Elementary functions
-------------------------------------------------------------------------------

* Add more transcendental functions.

* Double-check error bounds used in the fixed-point exponential code

* Faster elementary functions at low precision (especially log/arctan).
  Use Brent's algorithm (http://maths-people.anu.edu.au/~brent/pd/RNC7t4.pdf):
  atan(x) = atan(p/q) + atan((q*x-p)/(q+p*x))

* Use the complex Newton iteration for cos(pi p/q) when appropriate.
  Double check the proof of correctness of the complex Newton iteration
  and make it work when the polynomial is not exact.

* For small cos(pi p/q) and sin(pi p/q) use a lookup table of the
  1/q values and then do complex binary exponentiation.

* Investigate using Chebyshev polynomials for elefun_cos_minpoly.
  This is certainly faster when n is prime, but might be faster for all n,
  at least if implemented cleverly.


Special functions
-------------------------------------------------------------------------------

* Write a faster logarithmic rising factorial (with correct branch
  cuts) for reducing the complex log gamma function. Also implement
  the logarithmic reflection formula.

* Tune zeta parameter selection. Also make it robust: for example,
  if FMPRB_RAD_PREC is changed to 2 (a testing tactic suggested
  by Paul Zimmermann), the parameter selection code currently hangs
  in an infinite loop for many inputs.

* Extend Stirling series code to compute polygamma functions (i.e. starting
  the series from some derivative), and optimize for a small number of
  derivatives by using a direct recurrence instead of binary splitting.

* Fall back to the real code when evaluating gamma functions (or their
  power series) at points that happen to be real

* Implement more functions: error functions, Bessel functions,
  theta functions, etc.

Other
-------------------------------------------------------------------------------

* Document fmpz_extras

